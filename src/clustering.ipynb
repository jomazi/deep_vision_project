{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms, models\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import os.path\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fcts.load_data import load_data\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch size\n",
    "bs = 128\n",
    "# GPU or CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IconDataset(Dataset):\n",
    "    def __init__(self, part='train', transform=None):\n",
    "        \"\"\"\n",
    "        part: get either train or test set; string\n",
    "        transforms = transformations, that should be applied on dataset\n",
    "        \"\"\"\n",
    "        self.data = load_data(part=part)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index]\n",
    "        # transform image \n",
    "        if self.transform is not None:\n",
    "            img_transformed = self.transform(img)\n",
    "        # return transformed image\n",
    "        return img_transformed\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define transformations\n",
    "trafo_train = transforms.Compose([transforms.ToTensor()])\n",
    "trafo_test = transforms.Compose([transforms.ToTensor()])\n",
    "# datasets\n",
    "train_set = IconDataset(transform=trafo_train)\n",
    "test_set = IconDataset(part='test', transform=trafo_test)\n",
    "# dataloader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_baseline(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_dim):\n",
    "        super(Encoder_baseline, self).__init__()\n",
    "        \"\"\"\n",
    "        :param layer_sizes: list of sizes of layers of the encoder; list[int]\n",
    "        :param latent_dim: dimension of latent space, i.e. dimension out output of the encoder; int\n",
    "        \"\"\"\n",
    "        # store values\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        # initialize layers\n",
    "        layer_list = []\n",
    "        for in_dim, out_dim in zip(layer_sizes, layer_sizes[1:]):\n",
    "            layer_list.append(nn.Linear(in_dim, out_dim))\n",
    "            layer_list.append(nn.ReLU())\n",
    "        \n",
    "        # store layers\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "        \n",
    "        # layers for latent space output\n",
    "        self.out_mean = nn.Linear(layer_sizes[-1], latent_dim)\n",
    "        self.out_var = nn.Linear(layer_sizes[-1], latent_dim)\n",
    "    \n",
    "    def forward(self, x):  \n",
    "        \"\"\" \n",
    "        :param x: tensor of dimension (batch_size, 3, 32, 32)\n",
    "        :return means: tensor of dimension (batch_size, 3, latent_dim)\n",
    "        :return log_var: tensor of dimension (batch_size, 3, latent_dim)\n",
    "        \"\"\"\n",
    "        \n",
    "        # flatten x\n",
    "        x = x.view(-1, 3, 32*32)\n",
    "        \n",
    "        # forward \n",
    "        out = self.layers(x)\n",
    "        \n",
    "        # latent space output\n",
    "        means = self.out_mean(out)\n",
    "        log_vars = self.out_var(out)\n",
    "     \n",
    "        return means, log_vars\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder_baseline(nn.Module):\n",
    "\n",
    "    def __init__(self, layer_sizes, latent_dim):     \n",
    "        super(Decoder_baseline, self).__init__()\n",
    "        \"\"\"\n",
    "        :param layer_sizes: list of sizes of layers of the decoder; list[int]\n",
    "        :param latent_dim: dimension of latent space, i.e. dimension of input of the decoder; int\n",
    "        \"\"\"\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        layer_list = [nn.Linear(latent_dim, layer_sizes[0])]\n",
    "        \n",
    "        # initialize layers\n",
    "        for in_dim, out_dim in zip(layer_sizes, layer_sizes[1:]):\n",
    "            layer_list.append(nn.ReLU())\n",
    "            layer_list.append(nn.Linear(in_dim, out_dim))\n",
    "\n",
    "        # store layers\n",
    "        self.layers = nn.Sequential(*layer_list)\n",
    "            \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        :param z: tensor of dimension (batch_size, 3, latent_dim)\n",
    "        :return x: mu of gaussian distribution (reconstructed image from latent code z)\n",
    "        \"\"\"\n",
    "\n",
    "        # forward\n",
    "        x = torch.sigmoid(self.layers(z)).view(-1, 3, 32, 32)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_baseline(nn.Module):\n",
    "\n",
    "    def __init__(self, inp_dim, encoder_layer_sizes, decoder_layer_sizes, latent_dim):\n",
    "        \"\"\"\n",
    "        :param inp_dim: dimension of input; int\n",
    "        :param encoder_layer_sizes: sizes of the encoder layers; list\n",
    "        :param decoder_layer_sizes: sizes of the decoder layers; list\n",
    "        :param latent_dim: dimension of latent space/bottleneck; int\n",
    "        \"\"\"\n",
    "        \n",
    "        super(VAE_baseline, self).__init__()\n",
    "        \n",
    "        self.latent_dim = latent_dim\n",
    "        \n",
    "        self.encoder = Encoder_baseline(encoder_layer_sizes, latent_dim)\n",
    "        self.decoder = Decoder_baseline(decoder_layer_sizes, latent_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\" \n",
    "        :param x: tensor of dimension (batch_size, 3, 32, 32)\n",
    "        :return recon_x: reconstructed x\n",
    "        :return means: output of encoder\n",
    "        :return log_var: output of encoder (logarithm of variance)\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        x = x.view(-1, 3, 32*32)\n",
    "        means, log_vars = self.encoder(x)\n",
    "        std = torch.exp(.5*log_vars)\n",
    "        eps = torch.randn_like(std)\n",
    "        z = means + eps*std\n",
    "        recon_x = self.decoder(z)\n",
    "\n",
    "        return recon_x, means, log_vars\n",
    "        \n",
    "    def sampling(self, n=2):\n",
    "        \"\"\"\n",
    "        :param n: amount of samples (amount of elements in the latent space); int\n",
    "        :return x_sampled: n randomly sampled elements of the output distribution\n",
    "        \"\"\"\n",
    "        \n",
    "        # draw samples p(z)~N(0,1)\n",
    "        z = torch.randn((n, self.latent_dim))\n",
    "        # generate\n",
    "        x_sampled = self.decoder(z)\n",
    "\n",
    "        return x_sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE_advanced(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VAE_advanced, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.conv4 = nn.Conv2d(32, 16, kernel_size=3, stride=2, padding=1, bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(16)\n",
    "\n",
    "        self.fc1 = nn.Linear(8 * 8 * 16, 512)\n",
    "        self.fc_bn1 = nn.BatchNorm1d(512)\n",
    "        self.fc21 = nn.Linear(512, 512)\n",
    "        self.fc22 = nn.Linear(512, 512)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc3 = nn.Linear(512, 512)\n",
    "        self.fc_bn3 = nn.BatchNorm1d(512)\n",
    "        self.fc4 = nn.Linear(512, 8 * 8 * 16)\n",
    "        self.fc_bn4 = nn.BatchNorm1d(8 * 8 * 16)\n",
    "\n",
    "        self.conv5 = nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv6 = nn.ConvTranspose2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn6 = nn.BatchNorm2d(32)\n",
    "        self.conv7 = nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1, bias=False)\n",
    "        self.bn7 = nn.BatchNorm2d(16)\n",
    "        self.conv8 = nn.ConvTranspose2d(16, 3, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def encode(self, x):\n",
    "        conv1 = self.relu(self.bn1(self.conv1(x)))\n",
    "        conv2 = self.relu(self.bn2(self.conv2(conv1)))\n",
    "        conv3 = self.relu(self.bn3(self.conv3(conv2)))\n",
    "        conv4 = self.relu(self.bn4(self.conv4(conv3))).view(-1, 8 * 8 * 16)\n",
    "\n",
    "        fc1 = self.relu(self.fc_bn1(self.fc1(conv4)))\n",
    "        return self.fc21(fc1), self.fc22(fc1)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        fc3 = self.relu(self.fc_bn3(self.fc3(z)))\n",
    "        fc4 = self.relu(self.fc_bn4(self.fc4(fc3))).view(-1, 16, 8, 8)\n",
    "\n",
    "        conv5 = self.relu(self.bn5(self.conv5(fc4)))\n",
    "        conv6 = self.relu(self.bn6(self.conv6(conv5)))\n",
    "        conv7 = self.relu(self.bn7(self.conv7(conv6)))\n",
    "        return self.conv8(conv7).view(-1, 3, 32, 32)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return self.decode(z), mu, logvar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the loss function for the VAE\n",
    "def vae_loss(recon_x, x, mu, log_var, loss_func):\n",
    "    \"\"\"\n",
    "    :param recon_x: reconstruced input\n",
    "    :param x: input\n",
    "    :param mu, log_var: parameters of posterior (distribution of z given x)\n",
    "    :loss_func: loss function to compare input image and constructed image\n",
    "    \"\"\"\n",
    "\n",
    "    recon_loss = loss_func(recon_x, x)\n",
    "    kl_loss = torch.mean(0.5 * torch.sum(\n",
    "        torch.exp(log_var) + mu**2 - 1. - log_var, 1))\n",
    "    return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the Loss function for the VAE/CVAE\n",
    "def loss_function(recon_x, x, mu, log_var):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        recon_x: reconstruced input\n",
    "        x: input,\n",
    "        mu, log_var: parameters of posterior (distribution of z given x)\n",
    "    \"\"\"\n",
    "    ################################\n",
    "    # TODO: YOUR CODE STARTS BELOW #\n",
    "    ################################\n",
    "    \n",
    "    sigma_g = 1.\n",
    "    \n",
    "    neg_ELBO =0.5 * (torch.sum( mu.pow(2)+log_var.exp()-log_var-1 )+\n",
    "                       torch.sum( (x-recon_x).pow(2)/sigma_g**2. ) )\n",
    "    \n",
    "    print(neg_ELBO)\n",
    "    \n",
    "    return neg_ELBO\n",
    "    ################################\n",
    "    #     YOUR CODE ENDS HERE      #\n",
    "    ################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training of the VAE\n",
    "def train(model, epochs, path, optimizer, loss_fct):\n",
    "    \n",
    "    # check for previous trained models and resume from there if available\n",
    "    try:\n",
    "        previous = max(glob.glob(path + '/*.pth'))\n",
    "        print('load previous model')\n",
    "        checkpoint = torch.load(previous)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        loss = checkpoint['loss']\n",
    "        epochs_trained = checkpoint['epoch']\n",
    "    except Exception as e:\n",
    "        print('no model to load')\n",
    "        epochs_trained = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in np.arange(epochs_trained, epochs): \n",
    " \n",
    "        train_loss = 0\n",
    "        for batch_idx, data in enumerate(tqdm(train_loader, desc=f'Train Epoch {epoch}', leave=False)):\n",
    "            x = data\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            recon_batch,  mu, log_var = model(x)\n",
    "            loss = vae_loss(recon_batch,  x, mu, log_var, loss_fct)\n",
    "            #loss = loss_function(recon_batch,  x, mu, log_var)\n",
    "\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('====> Epoch: {} Average loss: {:.4f}'.format(epoch, train_loss / len(train_loader.dataset)))\n",
    "\n",
    "        # save model\n",
    "        torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                }, path+('/vae-{}.pth').format(epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "encoder_layer_sizes = [32*32, 512, 256]\n",
    "decoder_layer_sizes = [256, 512, 32*32]\n",
    "\n",
    "latent_dim_baseline = 2 \n",
    "vae_baseline = VAE_baseline(inp_dim=(32*32), encoder_layer_sizes=encoder_layer_sizes, decoder_layer_sizes=decoder_layer_sizes, latent_dim=latent_dim_baseline)\n",
    "vae_baseline = vae_baseline.to(device)\n",
    "optimizer_baseline = optim.Adam(vae_baseline.parameters(), lr=1e-3)\n",
    "\n",
    "loss_func_baseline = nn.MSELoss()\n",
    "\n",
    "epochs_baseline = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no model to load\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Epoch 0', max=2660, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 0 Average loss: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Epoch 1', max=2660, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.0009\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4e932edfc54b74b602fd1e6dbca5d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Epoch 2', max=2660, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-50e2303ff963>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/clustering/baseline'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_baseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func_baseline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ca90c6bf9046>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, path, optimizer, loss_fct)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#loss = loss_function(recon_batch,  x, mu, log_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/deep_vision_project/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/deep_vision_project/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(vae_baseline, epochs_baseline, './models/clustering/baseline', optimizer_baseline, loss_func_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advanced Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "latent_dim_advanced = 2 \n",
    "vae_advanced = VAE_advanced()\n",
    "vae_advanced = vae_advanced.to(device)\n",
    "optimizer_advanced = torch.optim.SGD(vae_advanced.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "loss_func_advanced = nn.MSELoss()\n",
    "\n",
    "epochs_advanced = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load previous model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7cf89f3ff5e474e84c1a857bfe8f871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Train Epoch 3', max=2660, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-87ca163e8fb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvae_advanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs_advanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/clustering/advanced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_advanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func_advanced\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-ca90c6bf9046>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, epochs, path, optimizer, loss_fct)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#loss = loss_function(recon_batch,  x, mu, log_var)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/deep_vision_project/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OneDrive/deep_vision_project/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(vae_advanced, epochs_advanced, './models/clustering/advanced', optimizer_advanced, loss_func_advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path2model = max(glob.glob('./models/clustering/advanced' + '/*.pth'))\n",
    "best_model = vae_advanced\n",
    "#best_model_dict = torch.load(path2model)\n",
    "#best_model.load_state_dict(best_model_dict['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(img):\n",
    "    plt.imshow(img)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "_, x= next(enumerate(train_loader))\n",
    "samples = x.to(device)\n",
    "samples_rec,   _, _ = best_model(samples)\n",
    "samples_rec = samples_rec.detach().cpu()\n",
    "for i in range(0, 3):\n",
    "    plt.subplot(3,2,2*i+1)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples[i].permute(1, 2, 0))\n",
    "    plt.title(\"Ori. {}\".format(i))\n",
    "\n",
    "    plt.subplot(3, 2, 2*i+2)\n",
    "    plt.tight_layout()\n",
    "    imshow(samples_rec[i].permute(1, 2, 0))\n",
    "    plt.title(\"Rec. {}\".format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- compare different loss functions; in paper they use L2 loss\n",
    "- use other architectures\n",
    "- tune hyperparameters; latent space dimensions\n",
    "- GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained ResNet from Pytorch\n",
    "resnet = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 1000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# original output for 1000 ImageNet classes\n",
    "resnet(samples).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop last layer to get images represented by the last layer\n",
    "modified_resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 2048, 1, 1])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modified_resnet(samples).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Icons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icon_features = np.array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, data in enumerate(tqdm(train_loader, desc=f'Encode Train Set', leave=False)):\n",
    "    x = data\n",
    "    x = x.to(device)\n",
    "    features = modified_resnet(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- other network: Code: https://github.com/KaimingHe/deep-residual-networks (ResNet-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_vision_project",
   "language": "python",
   "name": "deep_vision_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
